## 팀 구성원, 개인 별 역할

- CTO : 김재근
- 기획 : 박기문
- PM : 안지영
- 개발 : 이상준, 이현주
- 인프라 : 최민준

---

## 팀 내부 회의 진행 회차 및 일자

- 1차 회의 : `2023.01.16`
    - Discord를 통한 회의 진행
        - 김재근 (지각)
- 2차 회의 : `2023.01.17`
    - Discord를 통한 회의 진행
- 3차 회의 : `2023.01.18`
    - Discord를 통한 회의 진행
- 4차 회의 : `2023.01.19`
    - Discord를 통한 회의 진행
- 5차 회의 : `2023.01.20`
    - Discord를 통한 회의 진행

---

## 현재까지 개발 과정 요약

현재까지 개발을 진행하면서 ‘기술적으로 새로 알게된 점, 어려웠던 점, 아쉬운 점' 등을 요약하여 작성해 주세요 🙂

- 팀원 각자 현재 구현하고 있는 것을 적어주세요. :)

### 김재근님

> #### Task
> - Data Crawling 작업
>   - [x] Crawling Base Line 작성
>   - [x] 전시 상세페이지의 상단 데이터 크롤링 ( 전시회 명, 장소, 기간, 관람연령, 가격 )
>   - [x] 전시 상세페이지의 하단 예매자 통계 데이터 크롤링 ( 성별 통계, 연령별 통계 )
>   - [x] 전체적인 크롤링 자동화
> - Python으로 크롤링한 Image를 업로드 해둘 S3 연동작업 ( 진행 중 )

### 박기문님
 
> #### Task
> 1. 전시회 목록 조회
> - 전시회 목록 전체 조회 , 전시회 목록 한개 조회 , 목록 전체 조회하는 테스트코드 작성
> - ExihibitionEntity, ExhibitionResponse, ExhibitionController, ExhibitionException, ExhibitionRepository, ExhibitionService, ExhibitionControllerTest
> - `InProgress` : 크롤링해온 전시회 데이터 파싱하는 로직 구현
> ### 새로 알게 된 점
> - 깃허브를 통한 협업
>   - 프로젝트를 브랜치를 이용해 별도로 작업하고 PR을 보내는 방법을 새로 알게 되었다.
> - 프로젝트 설계 항목 준수
>   - JPA쓰면 좋겠지라고 생각해서 구현했었다.. 충돌이 날 수 있다는 생각을 하지 못했다. 설계 항목(프로젝트 설정)을 숙지하자.
> - 스프링 시큐리티는 너무 강력하다.
>   - 하다못해 API 호출 하나 하는 것 조차도 토큰을 넣어줘야 한다.
> ### 어려웠던 점
> - 기존 프로젝트에서는 Dto를 사용했었는데 Entitiy와 response만을 이용하려니 어려움이 많았었다.
> - findAll이 스프링에 내장되어있는줄 모르고 repository에서 findAll을 구현해서 충돌이 났었다.

### 안지영님

> #### Spring Security
> - Spring Security를 활성화하고 토큰 검증 필터로 JwtFilter를 추가하였습니다.
> - CustumAuthenticationEntrypoint와 CustomAccessDeniedHandler를 작성하여 각각 필터 발생시, 접근 제한시 에러 반환을 해주도록 했습니다.
> - 테스트코드를 작성하여 검증도 완료하였습니다.

### 이상준님

> #### Task
> ## 현재까지 개발 과정 요약
> - 전시 상세 조회  
전시 목록에서 id값을 통해 1개의 전시를 상세 조회할 수 있는 기능을 구현하였습니다.  
받아오는 내용으로는 전시이름, 시작시간, 종료시간, 가격, 관람연령제한, 상세정보, 갤러리상세정보, 갤러리id가 있습니다.
또한 성공적으로 1개의 전시를 상세 조회하는지 테스트하는 테스트코드를 작성하였습니다.  
> - 북마크 추가 기능  
유저가 원하는 전시를 북마크에 추가할 수 있는 기능을 구현하였습니다.  
한 번 누르면 북마크에 추가되고, 다시 한 번 누르면 북마크에서 삭제될 수 있게끔 로직을 만들었습니다.  
북마크 추가 기능에 대한 테스트코드는 총 3가지로 구성해 보았습니다.  
>   1. 북마크 추가 성공
>   2. 북마크 추가 실패 - 해당 게시물이 존재하지 않는 경우
>   3. 북마크 추가 실패 - 로그인 되지 않은 경우
> 
> 다만 테스트코드 내부적으로 문제가 있는 것으로 파악되어 현재 오류를 잡고 다시 테스트를 해보려는 중에 있습니다.
> #### 개발 과정에서 나왔던 질문(및 어려웠던 점)
> 
> 코드 내부적으로는 앞선 개인프로젝트와 수업내용과 유사해서 큰 어려움은 없었지만,  
> Git을 활용하여 협업을 하는 것에 있어서 처음이라 어려움을 겪었습니다. 구글링과 팀원들의 도움을 받아서 잘 해결할 수 있었습니다.

---

### 이현주님

> #### Task
> - Data Crawling
> - 인터파크 전시 정보 데이터 추출
>   1. 전시정보 text
>   2. 공지사항 image
>   3. 공지사항 text
>   4. 상세정보 image 주소
>
> #### 기술적으로 새로 알게된 점
>   - 개발 협업 전략 (**Git Stash)**
> 
>     재근님과 크롤링을 담당하면서 하나의 웹 페이지에서 추출할 항목들을 나누었다.
>     브랜치 전략에 익숙하지 않아 초반에 main에서 작업을 하는 바람에 conflict를 경험하였다.
>     conflict를 해결하기 위해 `git stash` 명령어를 활용하는 방법을 알게 되었고, 다행히 코드를 복구할 수 있었다.
> 
>   - 소통을 통한 학습
>     협업을 하면서 팀원들로부터 많은 것 (더 나은 작업 방식, 효율적인 유틸리티 사용 등)을 배울 수 있었다.
> 
>   - 웹 크롤링을 통한 데이터 추출
>     - 파이썬과 함께, 웹 크롤링에 사용되는 다양한 extension, library, module 등 다양한 생태계를 접했다. 다양한 생태계를 접하며 혼란스러웠지만 실제 프로젝트를 진행하면서 모르는 부분은 영문 공식 문서로 확인하며 배우는 것임을 알게 되었다. (이것이 정석인듯..)
>       - `library`: `BeautifulSoup`, `Soup Sieve`, `Selenium`
> #### 어려웠던 점
>   - **뮤지엄 메이트**는 산재하는 전시 정보 데이터를 하나로 모아 최선의 성능으로 정보를 제공하는 것이 목적이다. 여기서 산재하는 전시 정보 데이터 수집을 위해 `웹 사이트 크롤링`이라는 방법을 택했다.
>     - 크롤링에 능숙한 CTO님의 1:1교습을 통해 웹 사이트(인터파크)에서 전시 데이터를 모으는 것까지는 해냈다. 하지만 추출한 데이터를 프로젝트에 사용하기 위해 `적절한 스트링 형태로 파싱하는 과정`이 생각보다 많이 어려웠던 것 같다.
>     - 데이터를 다루는 것에서 가장 기본은 자료형의 특성을 이해하고 하나의 자료형으로 만들기 위해 적절한 메서드를 사용하는 것인데, 이건 기본기가 부족했던 것 같다.
> #### 아쉬운 점
>   - 각자 기능을 맡아서 작업을 진행하고 있다.  
    분업화는 효율적 작업 진행이 가능하다는 장점이 있지만, 다른 팀원이 작성한 코드를 익히는 시간이 필요하여 아쉬움이 남는다.  
    → 협업 시 타 팀원이 작성한 코드에 익숙해지는 시간을 줄이는 방법이 있는지 궁금하다.  
>   - ~~조 이름따라 되고 있다..네이밍의 중요성..~~

### 최민준님

> 1. ci/cd
> - 우리팀은 github 사용 경험 및 ci/cd를 직접 구축해보고 싶어서 gitlab이 아닌 github-action을 통해서 cicd를 구현했다.
> - 대략적인 환경은 github-action을 통해 main 브랜치에 merge가 되면 was를 빌드해서 docker-hub에 올린 뒤 docker에 접속해서 pull 받은 뒤 docker-compose를 통해서 spring,nginx를 같이 cd 하는 방식으로 진행했다.
> 2. 로그인 기능
> - 기존 accesstoken의 보안 취약점으로 인해서 redis + refresh 토큰을 활용하는 방법을 도입했다.
> - 로그인 시 accesstoken(5분),refreshtoken(30분) 2개의 토큰을 발급해준다
> - 이후 accesstoken이 만료 되면 redis에 저장되어있는 refreshtoken을 통해서 accesstoken을 재발급 한다
> 3. 로그아웃 기능
> - redis에 있는 refresh token을 삭제하고 인증에 사용한 acceess token을 블랙리스트에 등록해서 token을 사용못하게 하는 방식으로 로그아웃을 구현했다

---

## 개발 과정에서 나왔던 질문 (최소 200자 이상)

개발을 진행하며 나왔던 질문 중 핵심적인 것을 요약하여 작성해 주세요 🙂

- 질의응답 과정 중 해결되지 않은 질문을 정리하여도 좋습니다.

### 김재근님

> - 크롤링은 기본적으로 예전에 했던 지식을 바탕으로 했는데, 이것을 주기적으로 실행해서 DB에 적재하고 싶은 생각이 듭니다.  
하지만 지속적으로 크롤링하게되면 중복되는 데이터가 계속 쌓이게 될텐데 이 부분을 어떻게 해결해야할지 고민입니다.
> - 크롤링한 데이터를 DB에 적재하는 작업을 하고 있는데, 크롤링한 데이터를 List별로 원하는 테이블에 적재하고 싶은데 이 부분을 어떻게 해야할지 고민입니다.
> - 현재 S3에 이미지 데이터를 적재하는 작업 또한 진행 중에 있는데 파일 하나를 올리는 코드는 작성했으나, 폴더를 올리는 방법을 아직 찾지 못했습니다.

### 이현주님
> - `데이터 추출`과 `가공 단계`의 작업 경계선을 정하는 것이 어렵다.  
  크롤링 단계에서 최대한 데이터를 깔끔하고 보기 좋게 추출하는 것이 맞다고 생각했는데, 비효율적인 것 같아서 고민이다.  
> - 크롤링 과정을 주피터 노트북의 블록으로 쪼개어 `테스트, 세부 메서드, 통합 메서드`로 작성하였다.  
  run했을 때 데이터 추출이 잘 진행되다가 이후 통합 메서드에서 `UnboundLocalError`가 발생하였다.  
  할당되기 전에 변수를 호출했다는 에러인 것 같은데, 구글링을 통해 변수를 global로 선언하여 에러는 해결하였다.  
  하지만 파이썬의 `변수 스코프 개념`이 어려워서 완전히 이해하지는 못했다.

---

## 개발 결과물 공유

- Github Repository URL: [Museum-Mate](https://github.com/orgs/Museum-Mate/repositories)
- Team Notion URL: [Museum-Mate](https://geuun.notion.site/7aee403e2e0e4ac6b291abbb214886c0)
- Service URL: [Museum-Mate](http://www.withmuma.com:8080/swagger-ui/#)
- Crawling Data: [csv](https://github.com/Museum-Mate/data-crawling/blob/main/notebook/lhj/exhibition_230120_025900.csv)